\documentclass{letter}
\usepackage{color,soul}
\usepackage{hyperref}
\usepackage{fullpage}
\usepackage{cleveref}

\address{Clark Center S295 \\ Stanford University \\ Stanford, CA, 94305}
\signature{Robert T. McGibbon, Christian R. Schwantes, Vijay S. Pande}

\newcommand{\separate}{\begin{center}--------\end{center}}

\begin{document}

\begin{letter}{Prof. Sharon Hammes-Schiffer \\ Deputy Editor of JPC B \\ The Journal of Physical Chemistry}
\opening{Editor:}

Reviewer 1
\begin{quote}
In ``Optimal parameter selection in Markov state models for biomolecular conformational dynamics'', the authors elaborate on an earlier likelihood-based approach to the selection of the appropriate number of discrete states for crisp partitionings to attempt to obtain a Markov model that balances accuracy with the potential for overfitting that many-state models are capable of.  Three different critera are examined: a leave-some-out cross-validation likelihood estimate, as well as two criteria that penalize overfitting by using assumptions of asymptotic normality of the likelihood function (AIC and BIC).  Two different "emission probabilities" for the probability of observing a sample $x_t$ from a given discrete state s are considered that would produce a generative model---one that can be used to assign new data to a model: a uniform probability and Gaussian probability.  These are compared for consistency with each other, as well as compared with well-established metrics such as the stability of implied timescales of the model with the number of states.
\end{quote}

\begin{quote}
 This manuscript will be a useful contribution to the literature on Markov state model construction, but has a number of deficiencies that hinder publication in its current form.  Once these are addressed, it could be suitable for publication in JPC B.
\end{quote}

\begin{quote}
Most critically, the manuscript does not clearly explain the described work, fully justify the conclusions, or provide sufficient detail to reproduce the results.  A number of points below help address these deficiencies:
\end{quote}

\begin{quote}
The title ``Optimal parameter selection...'' does not reflect the work contained in the paper, which focuses only on the selection of the number of states.  Should the authors wish to keep the general title, more work on selecting other kinds of parameters should be included; otherwise, the title should be amended to something like ``Optimal selection of the number of states... using a likelihood approach...''
\end{quote}

This is a fair point, as we focus only on picking the number of states. In fact, we've changed the title to reflect this issue as well as the fact that we're not attempting to argue that our selection is truly ``optimal," but merely suggesting to use a method that is ``{\it optimizable}." \hl{reference the title.}

\separate
\begin{quote}
``As such, the eigenvalues, and correspondingly the relaxation timescales, should remain constant as the lag time increases.''  The eigenvalues of which matrix should remain constant with what parameter?  The eigenvalues of $T(t)$ actually change with t.  Perhaps you mean the eigenvalues of some implied rate matrix $K$ where $T(t) = exp[Kt]$?
\end{quote}

\begin{quote}
``choosing models whose relaxation timescales have converged''  Give definition for ``relaxation timescales''.
\end{quote}

The reviewer is exactly correct. We meant that the timescales will remain constant, but this means that the eigenvalues do change. We've corrected that sentence and also added a more detailed discussion of the transition probability matrix. The text can be found at the beginning of the section ``Prior Work."

\hl{add the text here too}
\separate
\begin{quote}
``With s(x) as the function mapping conformations into the indicator function basis set" Unclear: do you mean s(x) is the integer index of the basis function that is nonzero (has support) at x?  Please be precise.  The lack of precision in this manuscript substantially hampers readability and comprehension.
\end{quote}

We were too casual with our notation, but had meant that $s(\cdot)$ maps a conformation in phase space to an integer corresponding to the index of the state in the Markov model. This has been reworded to read:

\begin{quote}
With $s(\mathbf{x})$ as the function that maps a conformation in phase space ($\mathbf{x}$) to the index of a state in the discrete state space
\end{quote}
\separate
\begin{quote}
Eq. 1: This neglects a term for the initial conditions $x_0$.  Eq. 1 actually refers to the conditional probability $P[{x_t}|x_0]$.  Is this the quantity you intend to work with, rather than $P[{x_t}]$?  If so, why?
\end{quote}

This is most definitely true, but is not incredibly important. The reviewer is correct that technically we should include the probability of $x_0$ in the likelihood, however, when evaluating the relative likelihoods of models built on the same data, this term cancels out as it shows up in each model. Therefore, optimizing either function will arrive at the same result. \hl{We can add to the text if we think we need to, but this is really a nit-picky comment in my opinion.}

\separate
\begin{quote}
``By convention, we define $P(x | s(x))$ to be zero for all $x$ such that $s(x) \ne s_i$'' Be precise!  This statement is meaningless.  Do you mean ``we define $P(x | i) = 0$ whenever $s(x)\ne i$?''
\end{quote}

We've reworded this paragraph to be clearer. The text can be found right after Eq. (1) and is copied below:
\begin{quote}
With a discrete, non-overlapping state space, the likelihood of a sampled trajectory can be decomposed into a product of terms of two types: the state to state Markov transition probabilities, $T(s_i \rightarrow s_j) \equiv P(S_{t+1}=s_j | S_{t}=s_i)$, and so-called emission distributions of each state, $P(\mathbf{x} | s(\mathbf{x}))$, the conditional probability of observing a conformation at a given location in phase space given that the conformation, $\mathbf{x}$, is within a certain state, $s(\mathbf{x})$. As we are using  non-overlapping states, the conditional probability of observing a conformation, $\mathbf{x}$ from state $i$ is zero unless $\mathbf{x}$ is assigned to $i$. Mathematically, this means:
$$ P(\mathbf{x} | i) = 0 \;\;\;\; \forall \mathbf{x} \textnormal{ s.t. } s(\mathbf{x}) \neq i$$
\end{quote}

\separate
\begin{quote}
``We propose two possible emission distributions, $P(x | s(x))$''  Write this as $P(x | s)$ or $P(x | i)$, to make it clear that your generative probability model MUST be a probability over configurations $x$ given a discrete integer index $s$ or $i$.
\end{quote}

\begin{quote}
Eq. 3: Rewrite as $P(x|s)$ instead of $P(x_t|s(x_t))$, since these are supposed to be generative distributions for x given state index s.
\end{quote}
\hl{I'm not sure that this change is really all that necessary.}

\separate
\begin{quote}
``A more tractable alternative is a Gaussian emission model...''  Why is this even anticipated to be a good model?  Won't it force your states to have gaussianlike clusters in them?  And isn't a gaussian infinite in support, while you SPECIFICALLY REQUIRE the state membership functions to have finite support because they form a crisp partitioning?  Why are you not truncating this model at the state boundaries, which would affect your normalization of the probability density in Eq 5.  This inconsistency seems like a major flaw in this work.
\end{quote}

\begin{quote}
``We assume that the overlaps are small...''  Is this enforced by some condition on sigma and mu for neighboring states?  Has this assumption been checked?  This is certainly, absolutely, positively not the case for even the simple model in Fig 3A.
\end{quote}

\hl{This is one of the two major criticisms in the review so we should word the response carefully}

When we first added this emission model, our goal was to provide something that might be used in more realistic cases. As the volume calculation makes the volume emission model impractical for all but the simplest cases, we wanted a model whose probabilities could be evaluated analytically. One simple first guess would be to use a gaussian distribution. However, the reviewer brings up several issues with this model that make it too imprecise to be used in practice. As such, we've moved all of the Gaussian emission model results / discussion to an appendix section. It is still useful to think about as it is a stepping stone to using ``fuzzy" clustering or other methods whose state emission distributions may overlap.

\begin{quote}
On choice of emission probabilities, an obvious question: Why not choose $P(x|i) \propto exp[-\beta U(x)] \delta_{i,s(x)}$, where $U(x)$ is the potential energy and $\beta$ the inverse temperature?  The normalization constants are ALSO difficult to compute, but have the advantage that they are proportional to the stationary probability elements computed from the transition matrix $T$.
\end{quote}

This is certainly one possibility, and is desirable because it is physically motivated. \hl{maybe add a discussion in the gaussian section?}
\separate
\begin{quote}
``Unfortunately such approaches are intractable for problems of the size encountered in biomolecular simulations because of the need to integrate over all possible Markov models with a given number of parameters.''  Is this really intractable?  That's a very strong claim.  Perhaps it is just difficult or complex, since no evidence of intractability is presented?
\end{quote}
\hl{Is there some mathematical definition of intractability that I'm unaware of?}
In order to compute the Bayes factor for selecting the number of states, we would need to sample all transition matrices that can be built in $k$ states. This is simply impractical as you need to integrate over all $k \times k$ transition matrices.  

\separate
\begin{quote}
"N is the number of data points, assumed to be independent and identically distributed"  How reasonable is this assumption for molecular simulation data?  How do you even quantify the number of "iid normally distributed" data points in a molecular simulation?
\end{quote}

This is a subtle point, but is worth discussing. Since we are parameterizing a transition matrix, we choose the number of data points to be the number of transitions used to calculate the transition probabilities. However, as we are using the sliding window approach for counting transitions, we are in fact over-counting the number of identical transitions and so the BIC score is likely under-penalizing the models. This is a somewhat unanswered question in the field, however it has been discussed previously \cite{noe_sliding_window}. \hl{Do we want to add this to the main text?}.

\separate
\begin{quote}
``For cross validation, we find it essential to use a prior...''  Give complete equation for log likelihood.  How do you initialize and sample posterior?  Or maximize likelihood?
\end{quote}

We are being very unsophisticated in our use of the likelihood as we are not actually optimizing this likelihood to build our models. We are actually just using the likelihood to evaluate several models that are parameterized using the approach discussed in \cite{Beauchamp_msmb2}. \hl{Add this to the text, because the confusion is coming from the use of the word ``prior." We are not introducing a prior to the likelihood function, but merely using the prior when calculating the MAP for each model.}

\separate
\begin{quote}
``We simulated two trajectories with ... Langevin dynamics...''  Which langevin integration scheme was used!  How were the initial conditions selected?
\end{quote}
\hl{Need to look this up. I just used robert's code though.}

\separate
\begin{quote}
``The first trajectory was clustered using the k-centers clustering algorithm...'' Cite.  What parameters were used for clustering?
\end{quote}
\hl{Now we're getting really nit-picky, but fine.}

\separate
\begin{quote}
``The volumes did not change drastically...'' What does this mean, quantitatively?
\end{quote}

\begin{quote}
``...the likelihood remained essentially constant...'' What does this mean, quantitatively?
\end{quote}
\hl{It's really just a qualitative change. I just meant that the changes in the likelihood are much less than the differences between models, and so are not important for evaluating the ``best" model.}

\separate
\begin{quote}
according to the three methods" Which three methods exactly?
\end{quote}
We've clarified this to read:
\begin{quote}
according to the BIC, AIC, and test log-likelihood
\end{quote}

\begin{quote}
Fig. 2: This figure is way too small.  What is ``score''?  Is this a log likelihood?  Is lower or higher score representative of lower or higher likelihood?  The numbers/units of likelihood are actually meaningful, and should be shown.  What is the difference between "volume" and "Gaussian"?
\end{quote}
\hl{I disagree with this comment. The absolute likelihoods are basically meaningless, while the relative differences are important. I think adding units would be distracting.} 

\separate
\begin{quote}
Fig. 2 caption: ``form the boundaries of the optimal window''  Why is this region the optimal window?  It looks like the optimal BIC window is 100-200.  And why not just pick THE single optimal number of states?  Why do you need a window that contains highly nonoptimal models?
\end{quote}

We are not attempting to provide the ``best" model given the data, as there are many statistical tests for doing this. The window illustrated in these figures represents some reasonable range of models, and is a useful visualization of the differences in the three tests. In practice, however, one would choose one of the three and then pick the best model given that test. \hl{this was only mentioned in the captions of the figures, so I'll correct it when I change the figures.}

\begin{quote}
``The AIC and BIC penalize the larger state models much more than the test set log-likelihood...''  Why is this discrepancy so large?  Doesn't this suggest AIC/BIC are unsuitable if they are not representative of the actual out-of-sample performance on the test set?
\end{quote}

It's quite possible that the AIC and BIC are over-penalizing the likelihood. Given this, we would always suggest that one uses the test log-likelihood approach when possible. However, the ``best" models according to the AIC and BIC were close to the best model from the test log-likelihood, so they may still be useful when it is impossible to leave out any data during the parameterization step. We are by no means advocating that either the AIC or BIC are perfect approximations to the out-of-sample performance. \hl{Add a sentence saying this in the main text.}

\separate
\begin{quote}
``causes the transition matrix to suffer''  Clarify?
\end{quote}

We should not anthropomorphize our mathematical expressions! We've corrected this to read more clearly:
\begin{quote}
The decrease in the training set log-likelihood can be explained by a trade-off inherent in Eq. (1), where adding more states leads to an increase in the emission distribution term but causes the transition matrix term to decrease significantly more.
\end{quote}

\separate
\begin{quote}
For Mueller potential with Gaussian likelihood: Would be useful to show log-likelihood as a function of number of states so we can see how much the model prefers 5 states.
\end{quote}
\hl{I originally had this but you didn't like my graph. We could add it back or ignore this since it's moved to the appendix anyway}

\separate
\begin{quote}
Fig. 3: "Since k-means optimizes an objective function that is related to the emission probability" What is this objective function?  As I understand it, k-means simply minimizes the sum of intracluster variances, which has nothing to do with your choice of emission probability being uniform or Gaussian or otherwise. Or do you mean you optimize the log-likelihood form of Eq 1 using a voronoi decomposition based on generators that are optimized for each number of states?  Also: "The likelihood decreases as new states are added..."  Which likelihoods are printed in the figure panels?  Test set, uniform, or Gaussian emission probabilities?
\end{quote}

The k-means clustering method minimizes the average mean squared distance from each data point to its cluster's mean. This is precisely the same as the $\hat{\sigma}^2$ term in Eq. (8). When you write out the entire log-likelihood of the training dataset, there ends up being a term proportional to $\log \hat{\sigma}$. This is what we meant by the k-means clustering optimizing the likelihood function (and is precisely why Pelleg and Moore selected the probability that they did). \hl{Add clarification in figure to indicate that these are training set likelihoods.}

\separate
\begin{quote}
``...AIC is comparable to the test set log-likelihood...The BIC penalizes complexity more strongly..." Why the huge difference between the AIC and BIC?
\end{quote}

The difference just comes from the difference between the way both are defined. \hl{I want to brush this off as ``we're not advocating one or the other, but merely suggesting they may be useful in cases where cross-validation is not possible" but I'm not sure of the best wording.}

\separate
\begin{quote}
Fig. 4: Too small.  Missing figure title, like "Comparison of likelihood models for Fip35 WW domain trajectory data".  "Test set log-likelihoods are omitted in C as the dense matrix algebra is intractable for $k > 10^3$".  Intractable?  Really?  But this is just summing log $T_{ij}$ over N elements, isn't it?
\end{quote}
We really shouldn't use this word here, as it's not intractable, but it is difficult, as you have to fit the transition matrix with $k^2$ (nonzero) terms. Our software leverages sparse matrices, so it is difficult for us to actually parameterize large transition matrices that are not sparse.

\separate
\begin{quote}
``For tICA, this window is consistent with the convergence of the relaxation timescales"  So....looking at convergence of the relaxation timescales is just as good as the new proposed likelihood method?
\end{quote}

Yes it is. For well-sampled datasets built using the state of the art methods we can still use implied timecales, however, there are many times that the timescales are not as clean and easy to interpret. We've added a figure for models built with PCA instead of tICA on the WW dataset to illustrate how the implied timescales are not ``optimizable" in non-ideal cases. \hl{still need to make this figure.}

\separate
\begin{quote}
``Lane et al. proposed...whereas Kellogg et al...Consistent with this spread, we find the optimal number of states to be highly dependent on the space in which the data is clustered."  Were these two examples using the same metric of model quality and criteria for selecting the number of states?
\end{quote}
Lane et al. used an implied timescale approach with the RMSD metric, while Kellog et al. (discussed in the section ``Prior Work" \hl{right?}) used a likelihood model with a discriminative emission distribution. This line has been re-worked since we've moved the Gaussian emission model to the appendix.

\separate
\begin{quote}
``These results indicate why likelihood-based model selection is superior to approaches relying only on the timescale convergence.''  How?  I don't see clear evidence of this.  Which likelihood model is superior specifically?  How is this superior to choosing the smallest number of states for which the timescales become flat (assuming that such a thing exists)?  The only evidence presented here has judged success by examining concordance among AIC/BIC/leave-some-out cross validation and the timescale metric.  In Fig 4, BIC and AIC disagree, while AIC tracks (but does not quantitatively agree with) the test set method.  The agreement with the timescale convergence is shown as evidence of correctness.  But it seems like SOMETHING is missing to show that this approach (or, more properly, ONE of these likelihood approaches) is superior to simply checking when the timescales go flat.  A much more convincing test would be to show that these methods allow one to achieve smaller model error (eg in reproducing some quantitative features of the dynamics---maybe even the timescales?) for some smaller quantity of data when compared with a ``gold standard'' enormous dataset.  The behavior of the optimal number of states as a function of the quantity of data would also be of great interest.
\end{quote}

\hl{This is another important note that we need to answer carefully. The idea is that in ideal cases, our methods don't disagree with the timescale test drastically, but they are optimizable, which is in itself a significant advantage. This is particularly useful when the timescales simply do not converge, and cannot be quantitatively used to select a model. Secondly, the model that the statistical test selects may not be the model that best represents the protein of interest, however it does represent our best estimate of the system given how much data we have. Cross-validation is a rigorous way to compare models based on newly observed data, and so our test log-likelihood serves as a way to see that adding new states may increase the timescales, but in fact decreases the ``quality" of the model.}

\separate
\begin{quote}
``These results take a step toward fully automating the MSM construction process...by providing a quantitative method for selecting the most suitable complex model.''  Which method in particular is recommended?  The Gaussian method?  AIC or BIC?  I'm still confused about what the actual, concrete demonstration of superiority is and which method the authors suggest should be used.
\end{quote}

The advantage is precisely that this framework is {\it optimizable}. The statistical tools we've suggested match the Chapman-Kolmogorov when the dataset is ``well-behaved" (i.e. when we have a lot of data and are using tICA). However, there are many cases where the timescales do not converge at all and so the Chapman-Kolmogorov tests fail. We've illustrated this with a simple example (\hl{See section ``PCA on WW")}, but in fact there are many larger simulations that give rise to wildly miss-behaving implied timescales.

\separate
\begin{quote}
Minor issues that the authors should address are below:
Abstract: ``in regimes in which these techniques encourage over-fitting''  It is unclear precisely what ``these techniques'' refers to.
\end{quote}
This is a good point, and we've rephrased to say:
\begin{quote}
Application of techniques that consider both systematic bias and statistical error on two 100$\mu s$ molecular dynamics trajectories of the Fip35 WW domain shows agreement with existing techniques based on self-consistency of the model's relaxation timescales, with more suitable results in regimes in which those timescale-based techniques encourage over-fitting.
\end{quote} \hl{this sentence feels a bit long...}

\separate
\begin{quote}
Two key quantities which define the MSM are thus the state definitions, an indicator function basis over phase space..."  There is a substantial quantity of literature on the use of basis functions that are not indicator functions (i.e. "fuzzy" rather than "crisp" partitionings) that should be mentioned.  See, for example, the work of Marcus Weber and Susanna Roeblitz (nee Kube).
\end{quote}

\hl{We should add a section discussing this as well as the future of directly optimizing the likelihood in the form of an HMM or other type of moel}

\separate
\begin{quote}
The model has the training data set as its support rather than phase space, which makes it unable to generalize and assign probability to new data."  It may also be worth noting that this is not a generative model, since the next paragraph states that a generative model is desired.
\end{quote}

This is precisely what we are drawing attention to, and we've added to this section to explicitly say this is a discriminative model.

\begin{quote}
``but is intractable in high dimensions when the states are defined by general polytopes, such as those produced by a data-driven Voronoi tessellation''  Be clear that *exact* volume computation is intractable in very high dimension due to the complexity of all known algorithms.  Monte Carlo schemes (such as the one used here, or methods such as [http://www.cs.elte.hu/~lovasz/vol4-focs-tr.pdf]) allow the approximation of this volume.  However, it's also worth noting that the Gaussian model you propose in Eq. 4 is ALSO INTRACTABLE for EXACTLY THE SAME REASON, since you do not compute the true normalizing constant for essentially identical reasons of computational tractability in high dimension.  As such, phrases such as "A more tractable alternative is a Gaussian emission model.." are entirely disingenuous and misleading.
\end{quote}

The reviewer is exactly right. For these reasons (and others) we've chosen to discuss just the volume-based emission model in the main text. This is really what we'd like to focus on.

\begin{quote}
``All MSMs were built using a lag time of 50ns''  Why 50 ns?
\end{quote}

This was the same lag time that was used in Beauchamp et al.\cite{Beauchamp_shaw_paper}. In this scheme, the lag time must be specified before selecting the models, and so we picked a number that had been successfully used in the past. It's worth noting, however, that our scheme does not provide a framework for the selection of the lag time.
\separate
\begin{quote}
``identify the simulations folding process''  Something isn't quite right with this phrase.  What is being referred to?
\end{quote}

This sentence was poorly worded, but we've reworked it and copied the new version below:
\begin{quote}
While only using 200-600 states, Markov models constructed using tICA were still able to recapitulate the folding timescale as well as the faster processes identified by McGibbon and Pande.
\end{quote}

\begin{quote}
``the relaxation timescales can continually increase with the number of states used''  Even in the limit of an infinite quantity of data, the eigenvalue error bound theorems from Schuette et al. note that this behavior is precisely what we should expect, since the approximation error decreases as the eigenfunctions are better approximated via more states.  However, the statistical error grows, though this is not quantified or plotted here.
\end{quote}

The reviewer is exactly right, and this is the point we're trying to drive home. It would be nice to compute the statistical error, but it is a difficult thing to calculate. \hl{This is in the minor complaints section so I don't know if we really need anything more than this}

Reviewer 2:

\begin{quote}
The authors present an alternative approach for choosing the Markov time for Markov state models that will be of value for automating the construction of these models by removing the need for subjective user input.  I have a few concerns, below.
\end{quote}

\begin{quote}
Line 34: Did you mean sacrificing ``accuracy'' instead of ``complexity''?
\end{quote}

We do mean complexity here, however the two words go hand in hand. In this case, we're talking about an MSM built either with thousands vs. tens of states. The simpler model therefore cannot describe transitions between two states from the complicated model that are lumped together in the simple model. In this sense, the simpler model is less accurate. However, this may not matter, if what you really care about is a large global transition between ensembles of small states. In this sense, the simpler model does not lose any accuracy. \hl{I hate how I worded this.}

\separate
\begin{quote}
Can the authors comment on whether k-means or k-centers is preferable based on their Muller potential results?
\end{quote}

\hl{I have this data, so we can add it as an appendix. It doesn't hurt.}

\separate
\begin{quote}
The authors have access to many other data sets from their own lab and the Shaw group.  Would it be possible to show results for other systems (maybe one larger system and one system with more disorder)?  I think this would help to establish the generality of their results.
\end{quote}

This is a very good idea and something we're interested in doing. However, due to the complexity of the volume calculation it's simply not feasible. We hope that our work serves as a stepping stone to new methods that optimize a likelihood supported in phase space, but admit it may not be the one discussed here. 


Reviewer 3:

\begin{quote}
Optimal Parameter Selection in Markov State Models for Biomolecular Conformational Dynamics by McGibbon, Schwantes and Pande. I enjoyed reading this paper. It is well suited for Bill Swope's birthday issue. It addresses an important problem in chemical physics, namely choosing amongst the various possible parameter settings when constructing a Markov State Model. However, the following general points should be addressed in a revision:
\end{quote}

\begin{quote}
I find the title “Optimal Parameter Selection in Markov State Models...” inappropriate: It suggests that the ``Parameter selection in MSMs'' is a well-defined problem, and that the solution approach here would be optimal, i.e. not further improvable. Neither is true. I suggest to pick a title that is specific to the content of the paper, e.g. ``Likelihood-approach for selecting the number of states / choosing between discretizations in MSMs''
\end{quote}

\begin{quote}
The paper starts with a discussion of approaches that choose the lag time based on implied timescales, and then proposes a way to choose the number of discrete states as an alternative. I can see the idea behind it, but to general reader it will probably be unclear what the connection between the two is. It should be spelled out that there is a change of viewpoint, where instead of fixing the discretization and varying the lagtime, one fixes the lagtime and varies the discretization. And the hope why both approaches are meaningful is that the implied timescales should converge to their true values for either (1) increasing lag time [see Djurdjevac, Sarich, Schütte, MMS 2011 and Prinz, Chodera, Noe, PRX 2014 (http://arxiv.org/abs/1207.0225)], or (2) better approximation of the dominant eigenfunctions via more discrete states [see (19) and Sarich, Noe, Sch\"{u}tte MMS 8, p1154 (2010)].
\end{quote}

\begin{quote}
My main concern is that the paper suggests that the best result is obtained by maximizing the present likelihood. But stationary and dynamical expectation values such as the relaxation timescales are well defined by the underlying dynamics, and the ultimate objective of any estimation procedure must be to correctly approximate them. Using the present likelihood could systematically give wrong estimates for various reasons. 
\end{quote}

\begin{quote}
An inappropriate statistical model for the local output function (which is really hard to make) would lead to biased estimates. Then there is uncertainty in choosing the Bayesian criterion - AIC, BIC or something else? If the lag time chosen is too short, it will not lead to correct estimates for feasible numbers of states.
\end{quote}

\begin{quote}
The differences in Fig 4 (see Details below) indeed suggests that the present criterion fails to provide the correct estimate in at least some cases. If a new estimator is derived, it needs to be shown that it converges to the correct result in some limit. 
\end{quote}

\begin{quote}
I suggest to build a fine discrete model, e.g. by direct gridding of the M\"uller potential, and constructing a simple MCMC dynamics on neighboring gridpoints, and then considering different clusterings on this data set. There, the exact timescales are known, and an embedding in a geometric space is given, such that the method can be applied such that the present method can be applied as is. It then needs to be shown that the present likelihood approach selects MSMs with timescales that coincide with the true values.In addition, it would be useful to compare the results here with implied timescale plots. Do the results agree in the cases where ITS do converge?
\end{quote}

\begin{quote}
Please check the timescales in Fig 2(A,B) and 4(A,B) - they look identical, but shouldn't. Details / specific points:
\end{quote}

These timescales were correctly displayed. The timescales in A and B are calculated from the same set of models, but the evaluation of those models just provides a different ``score." Part B of each of these figures has been moved to the appendix.

\begin{quote}
``ever-more states''. Ever-more states or ever-larger lag time? (when did number of states come in?)
\end{quote}

\begin{quote}
``Often the choice is made to use the model whose timescales better match an experimental observable:`` I hope that's not true and I wouldn't generally accuse the community of practicing this. When timescales don't converge, this is a numerical or statistical problem with the data or the estimators used. Ignoring this fact and deliberately choosing a model that coincidentally delivers the numbers one hopes to get would be simply cheating.
\end{quote}

The reviewer is definitely correct here. We are not accusing anyone of ``cheating" but more drawing attention to the fact that with the absence of an optimizable objective function there is nothing preventing one from picking the model that gives a reasonable answer. \hl{we should rephrase this sentence, but I don't know what to do exactly}

\separate
\begin{quote}
Gaussians could be truncated and normalized - it seems that renormalization would require to solve an even harder problem than for uniform output probabilities, because you’d need to integrate the Gaussian density over a high-dimensional polytope.
\end{quote}

\begin{quote}
``We assume that the overlaps are small'' But are they small in your application? Ensuring this puts requirements on the sizes of states and sigma. How is it checked that this is then a good model?
\end{quote}

\begin{quote}
Are you sure that the timescales shown in Fig 2A(right) and 2B(right) are from different data? They look identical to me.
\end{quote}

\begin{quote}
I’m surprised that there are three slow relaxation processes for the Müller potential before the second gap. The potential looks like there should be only two (due to three metastable states). Were MSMs build with reversible transition matrix estimation, and are all eigenvalues thus real-valued?
\end{quote}

\begin{quote}
Muller potential: lag time was chosen to be 30 steps. How is this choice justified? Since the timescales in Fig. 2 are not independent of N, this must mean that at least for some state decompositions the ITS are not converged in the lag time. In these cases, it’s not even appropriate to describe the state-to-state dynamics by a Markov chain at all. Is the statistical model still meaningful in this case
\end{quote}

\begin{quote}
Müller potential, Gaussian likelihood: How were the Gaussian sigma-parameters chosen? The k-means states in Fig 3 have very different sizes and their centers are sometimes very close, so the assumption that output distributions shall not overlap seems to be hard to realize.
\end{quote}

\begin{quote}
WW domain, lag time was chosen to be 50 ns - same comment as for the Mueller potential, above
\end{quote}

\begin{quote}
WW domain, Gaussian likelihood - same comment as for the Müller potential, above
\end{quote}

\begin{quote}
Are you sure that the timescales shown in Fig. 4A(right) and 4B(right) are from different data? They look identical to me.
\end{quote}

\begin{quote}
 Doesn’t Fig 4c suggest that there’s a problem here? For the likelihood-based choice made, the slowest timescale is estimated between 1500 and 3000 ns with RMSD clustering. In contrast, the TICA-based estimates suggest a timescale of around 5000 ns. So if the TICA results is indeed correct, the method should be going for larger state number in the RMSD case.
\end{quote}

\begin{quote}
In some timescale vs number of state plots the timescales descreases with increasing number of states (e.g. Fig 2A). That’s surprising. Can the authors explain that? Has a prior been used that may be responsible for this behavior?
\end{quote}


\closing{Sincerely,}


\bibliography{bibliography.bib}
\end{letter}
\end{document}
